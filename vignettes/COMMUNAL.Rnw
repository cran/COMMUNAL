\documentclass[a4paper]{article}
\usepackage{a4wide}
\setlength{\parskip}{0.7ex plus0.1ex minus0.1ex}
\setlength{\parindent}{0em}

%\VignetteIndexEntry{COMMUNAL: A Robust Method for Selection of Cluster Number K}
%\VignetteKeyword{clustering}
%\VignettePackage{COMMUNAL}

\title{COmbined Mapping of Multiple clUsteriNg ALgorithms (COMMUNAL):
  A Robust Method for Selection of Cluster Number K: 
  R Package Vignette}
\author{Timothy E Sweeney \\ Stanford University \and 
        Albert Chen \\ Stanford University \and 
        Olivier Gevaert \\ Stanford University}

\begin{document}
\maketitle

\section*{Introduction}
This vignette describes one main usage of the \texttt{COMMUNAL} algorithm. For a fuller description of the method, please see: Sweeney TE, Chen AC, Gevaert O. ``COmbined Mapping of Multiple clUsteriNg ALgorithms (COMMUNAL): A Robust Method for Selection of Cluster Number K.'' Scientific Reports, 2015. Please also cite this paper if you use COMMUNAL in published work. 

\texttt{COMMUNAL} attempts to solve a vexing problem in unsupervised learning (clustering), namely, how to choose the `right' or 'optimal' number of clusters in a dataset. There are many methods available, but a review is outside the scope of this vignette. \texttt{COMMUNAL} has two basic functions. The first is the actual function COMMUNAL(). The COMMUNAL() function takes a data matrix as input, along with a range of K to test, and a set of clustering algorithms (such as k-means, hierarchical, etc.) and validity metrics (such as silhouette index, gap statistic, etc.). All algorithms are run on the data over the range of inputted K, and all validity metrics are applied to all algorithms. COMMUNAL() will then return the clustering results across the range of K. In general, we do not imagine most users will run the COMMUNAL() function directly.

The main algorithm which is described in the \texttt{COMMUNAL} paper actually begins with the function clusterRange(). clusterRange() is a handle that calls COMMUNAL() iteratively over progressive variable (row) subsets of data. By default, clusterRange() will sort the rows by decreasing order variance, and include the highest-variance rows first. This is modifiable by the user. The output from clusterRange() is used to locally optimize the choices of algorithms (throwing out those which choose too many very small clusters) and validity metrics (throwing out those which behave monotonically and are highly correlated). It will then use the remaining algorithms and validity metrics to produce a 3D plot of cluster optimality over the range of included variables. We suggest that this plot be used to inform a decision of which variables to use, and which K to pick. 

The last part of the \texttt{COMMUNAL} package deals with assigning samples to clusters when algorithms disagree over cluster assignments. We call the resulting method the `core' clustering. The assumption is that samples which are grouped together by multiple algorithms are truly similar, while those for which the algorithms all disagree are slightly noisy and may be outliers. 

Finally, we note that the clusterRange() function can also be used to call just a single algorithm and metric (e.g., k-means with gap statistic). This would still improve a `typical' clustering run by applying the algorithm and metric over a range of variable subsets, thus identifying stable optima for K. A 3D plot will still be produced using just one algorithm and validity metric -- go on, try it!

As an aside, we note that the \texttt{rgl} package, which is required by the 3D plotting function plotRange3D(), is somewhat finnicky. It's a fantastically useful package and we are incredibly grateful to its makers. However, you will have to seek advice directly from them should \texttt{rgl} throw errors on your machine. We recommend trying \texttt{COMMUNAL} with a small number of simple parameters as a test run prior to throwing a massive dataset with all algorithms and all validity metrics. You can even run the code from this vignette! If so, we recommend limiting the `varRange' function to just c(20, 40) to save yourself time. 

\section*{Tutorial}

\subsection*{Identifying $k$}
The \texttt{clusterRange} function provides a harness to \texttt{COMMUNAL} to test progressive subsets of variables. Here we load some breast cancer data (100 gene expression levels (rows) of 533 samples (columns)). We'll use progressive subsets of 20, 40, 60, 80, and finally all 100 variables (rows).

Note that here we pick a subset of validation measures that runs quickly; to run all measures, simply input option "all"; be forewarned that this can cause delays, especially the gap statistic (since it requires bootstrapping, although this is modifiable by the user), and the measure `g2'. 

Verbose is set to F; however, setting to TRUE is an effective way to see progress and find areas of delay. Note that the output may be disordered if the parallel option is used. 
<<>>=
library(COMMUNAL)
data(BRCA.100)
varRange <- seq(20,100,20)
ks <- 2:8
measures <- c("average.between", "dunn", "widestgap", "dunn2", 
                      "pearsongamma", "g3", "max.diameter", "avg.silwidth")

BRCA.results <- clusterRange(dataMtx=BRCA.100, ks = ks,
                              varRange=varRange, 
                              validation=measures,
                              verbose = F)
@

Now \texttt{BRCA.results} contains the results from running \texttt{COMMUNAL} repeatedly on the subsetted data. 

To get a set of locally optimized algorithms, we measure the percentage of clusterings for which a given algorithm returned any clusters with minSize or fewer members.
<<>>=
algs <- getGoodAlgs(BRCA.results, algs="all")

algs
@


We similarly locally optimize the validity metrics used by eliminating those that are monotone, and those that are highly correlated. The correlation step requires the user to pick a number of measures to keep-- the default is four.
<<>>=
monotoneClusterRange(BRCA.results)

measuresCorr(BRCA.results)

measures <- getNonCorrNonMonoMeasures(BRCA.results, goodAlgs=algs, numMeasures = 4)

measures
@

We can generate the plots using \texttt{plotRange3D}. These indicate that two clusters is best in this very limited toy example. First will be a 2d plot; this is a view of mean values collapsed for all variable subsets. Second will be a snapshot of the 3D plot; this is the main output, and is normally interactive and pops up in an extra screen. Plot3D has been set to false for this example, but a snapshot of example output is provided.

\begin{center}
<<fig=TRUE>>=
plot.data <- plotRange3D(BRCA.results, ks, algs, measures, plot3D=F)

## print the values from the 3D plot
plot.data
@
\end{center}

Below is a snapshot of the 3D plot. This shows the results before aggregation into the 2D plot above. The red dots mark the steepest non-edge peak, if a peak exists. The blue points mark the overall maximum for each variable set. The z-axis has labels for Tukey's five number summary of all the values. For an interactive 3D example of \texttt{plotRange3D}, you may run this code yourself or see the help page for \texttt{plotRange3D}.
\begin{center}
\includegraphics[width=0.75\textwidth]{snapshot.png}
\end{center}

\subsection*{Extracting Core Clusters}
After looking at these results, we are satisfied that $k = 2$ is optimal. However, solely for demonstration purposes, we will use $k=3$, since it allows for more interesting results below. The next step is to extract the cluster assignments for $k = 3$ with \texttt{getClustering}. It returns a data frame whose rows are the samples and columns are the clustering algorithm names. Each entry is the cluster assignment of a sample from the respective algorithm. This is the input used in identifying `core' clusters.

However, we first need to choose a variable subset which is optimal. One might choose the subset with the fewest variables where the target clustering is obtained (here 20 variables). 
<<>>=
result <- BRCA.results$all.results$vars_20
clusters <- result$getClustering(k=3)
apply(clusters, 2, table)
@

From the table, you can see that there are disagreements between clustering algorithms, but in general the clusters fall into three groups of about equal size (just as expected). Now we combine the results to get final cluster assignments. This shows that the algorithms agree on all but 21 of the assignments (which end up in cluster 0, meaning `unassigned')
<<>>=
# re-key cluster labels to most frequent assignments
mat.key <- clusterKeys(clusters)
examineCounts(mat.key)

# find 'core' clusters
core <- returnCore(mat.key, agreement.thresh=50) # find 'core' clusters
table(core) # the 'core' clusters
head(core) # the cluster assignments
@

Now let's consider a more involved example of how \texttt{clusterKeys} and \texttt{returnCore} are useful. Consider the following cluster assignments. Overall the algorithms agree that there are three clusters, but differ in how they label the clusters. They disagree about the cluster of the last point.
<<>>=
clusters.example <- data.frame(
  alg1=as.integer(c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,1)),
  alg2=as.integer(c(1,1,1,1,1,3,3,3,3,3,2,2,2,2,1)),
  alg3=as.integer(c(3,3,3,3,3,1,1,1,1,1,2,2,2,2,2))
)
@

\texttt{clusterKeys} reindexes the labels for each algorithm to make the agreement more apparent.
<<>>=
mat.key <- clusterKeys(clusters.example)
mat.key # cluster indices are relabeled
@

The next step is to synthesize these into ``core'' clusters. The clusters are assigned by majority vote. If not enough algorithms agree, based on a user-defined threshold, the cluster is left undetermined. \texttt{examineCounts} shows how many samples would be undetermined at various threshold levels.
<<>>=
examineCounts(mat.key)
@

Now we use a threshold to retrieve the ``core'' clusters. The default threshold is 50\%, meaning that more than 50\% of the algorithms must agree. In this case, if we use the 50\% threshold, then all points are assigned to some cluster.
<<>>=
core <- returnCore(mat.key, agreement.thresh=50) # find 'core' clusters
table(core) # the 'core' clusters
@

However, if we require all algorithms to agree, then one point is undetermined (hence labeled as cluster 0).
<<>>=
core <- returnCore(mat.key, agreement.thresh=99)
table(core) # 0 is undetermined
@

\end{document}